{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c19a42d-9a6a-4a32-979a-554e0071e158",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exploratory Data Analysis - Extract Data\n",
    "In this notebook we will practice extracting data into Spark dataframe and then store the data from Spark dataframe into Python/pandas dataframe and into databricks table for future use. Having the data into pandas dataframe will allow us to perform exploratory data analysis and visualization using python in the furure notebooks.\n",
    "\n",
    "We will perform following steps -\n",
    "- Extract data from CSV file into spark dataframe\n",
    "- Extract data from JSON file into spark dataframe\n",
    "- View data from spark dataframe\n",
    "- Load data from spark dataframe to pandas dataframe\n",
    "- View data from pandas dataframe\n",
    "- Load data from spark dataframe to databricks table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "809c964e-4408-48a9-8c22-f665193cc5f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Extract data from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59d5d458-cfc9-4442-974a-c4d3f3d22dfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The following code will read CSV file using Spark from DBFS into spark dataframe\n",
    "\n",
    "\"\"\" \n",
    "1. Passing True in the header option lets spark to treat the first row as the header of the dataframe, without that \n",
    "the columns will be named as c0, c1, and so on.\n",
    "2. Passing True in the inferSchema option lets spark to interpret the correct datatype of each column based on the \n",
    "data, without that all columns will be of string type.\n",
    "\"\"\"\n",
    "\n",
    "df1 = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .load(\"dbfs:/FileStore/DataFiles/SuperstoreOrders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7f35e6b-bf69-4f8d-b716-3fd1f916b7b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Row ID</th><th>Order ID</th><th>Order Date</th><th>Ship Date</th><th>Ship Mode</th><th>Customer ID</th><th>Customer Name</th><th>Segment</th><th>Country/Region</th><th>City</th><th>State/Province</th><th>Postal Code</th><th>Region</th><th>Product ID</th><th>Category</th><th>Sub-Category</th><th>Product Name</th><th>Sales</th><th>Quantity</th><th>Discount</th><th>Profit</th></tr></thead><tbody><tr><td>1</td><td>US-2020-103800</td><td>2020-01-03</td><td>2020-01-07</td><td>Standard Class</td><td>DP-13000</td><td>Darren Powers</td><td>Consumer</td><td>United States</td><td>Houston</td><td>Texas</td><td>77095</td><td>Central</td><td>OFF-PA-10000174</td><td>Office Supplies</td><td>Paper</td><td>\"Message Book Wirebound Four 5 1/2\"\" X 4\"\" Forms/Pg. 200 Dupl. Sets/Book\"</td><td>16.448</td><td>2</td><td>0.2</td><td>5.5512</td></tr><tr><td>2</td><td>US-2020-112326</td><td>2020-01-04</td><td>2020-01-08</td><td>Standard Class</td><td>PO-19195</td><td>Phillina Ober</td><td>Home Office</td><td>United States</td><td>Naperville</td><td>Illinois</td><td>60540</td><td>Central</td><td>OFF-BI-10004094</td><td>Office Supplies</td><td>Binders</td><td>GBC Standard Plastic Binding Systems Combs</td><td>3.54</td><td>2</td><td>0.8</td><td>-5.487</td></tr><tr><td>3</td><td>US-2020-112326</td><td>2020-01-04</td><td>2020-01-08</td><td>Standard Class</td><td>PO-19195</td><td>Phillina Ober</td><td>Home Office</td><td>United States</td><td>Naperville</td><td>Illinois</td><td>60540</td><td>Central</td><td>OFF-LA-10003223</td><td>Office Supplies</td><td>Labels</td><td>Avery 508</td><td>11.784</td><td>3</td><td>0.2</td><td>4.2717</td></tr><tr><td>4</td><td>US-2020-112326</td><td>2020-01-04</td><td>2020-01-08</td><td>Standard Class</td><td>PO-19195</td><td>Phillina Ober</td><td>Home Office</td><td>United States</td><td>Naperville</td><td>Illinois</td><td>60540</td><td>Central</td><td>OFF-ST-10002743</td><td>Office Supplies</td><td>Storage</td><td>SAFCO Boltless Steel Shelving</td><td>272.736</td><td>3</td><td>0.2</td><td>-64.7748</td></tr><tr><td>5</td><td>US-2020-141817</td><td>2020-01-05</td><td>2020-01-12</td><td>Standard Class</td><td>MB-18085</td><td>Mick Brown</td><td>Consumer</td><td>United States</td><td>Philadelphia</td><td>Pennsylvania</td><td>19143</td><td>East</td><td>OFF-AR-10003478</td><td>Office Supplies</td><td>Art</td><td>Avery Hi-Liter EverBold Pen Style Fluorescent Highlighters 4/Pack</td><td>19.536</td><td>3</td><td>0.2</td><td>4.884</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "US-2020-103800",
         "2020-01-03",
         "2020-01-07",
         "Standard Class",
         "DP-13000",
         "Darren Powers",
         "Consumer",
         "United States",
         "Houston",
         "Texas",
         "77095",
         "Central",
         "OFF-PA-10000174",
         "Office Supplies",
         "Paper",
         "\"Message Book Wirebound Four 5 1/2\"\" X 4\"\" Forms/Pg. 200 Dupl. Sets/Book\"",
         16.448,
         2,
         0.2,
         5.5512
        ],
        [
         2,
         "US-2020-112326",
         "2020-01-04",
         "2020-01-08",
         "Standard Class",
         "PO-19195",
         "Phillina Ober",
         "Home Office",
         "United States",
         "Naperville",
         "Illinois",
         "60540",
         "Central",
         "OFF-BI-10004094",
         "Office Supplies",
         "Binders",
         "GBC Standard Plastic Binding Systems Combs",
         3.54,
         2,
         0.8,
         -5.487
        ],
        [
         3,
         "US-2020-112326",
         "2020-01-04",
         "2020-01-08",
         "Standard Class",
         "PO-19195",
         "Phillina Ober",
         "Home Office",
         "United States",
         "Naperville",
         "Illinois",
         "60540",
         "Central",
         "OFF-LA-10003223",
         "Office Supplies",
         "Labels",
         "Avery 508",
         11.784,
         3,
         0.2,
         4.2717
        ],
        [
         4,
         "US-2020-112326",
         "2020-01-04",
         "2020-01-08",
         "Standard Class",
         "PO-19195",
         "Phillina Ober",
         "Home Office",
         "United States",
         "Naperville",
         "Illinois",
         "60540",
         "Central",
         "OFF-ST-10002743",
         "Office Supplies",
         "Storage",
         "SAFCO Boltless Steel Shelving",
         272.736,
         3,
         0.2,
         -64.7748
        ],
        [
         5,
         "US-2020-141817",
         "2020-01-05",
         "2020-01-12",
         "Standard Class",
         "MB-18085",
         "Mick Brown",
         "Consumer",
         "United States",
         "Philadelphia",
         "Pennsylvania",
         "19143",
         "East",
         "OFF-AR-10003478",
         "Office Supplies",
         "Art",
         "Avery Hi-Liter EverBold Pen Style Fluorescent Highlighters 4/Pack",
         19.536,
         3,
         0.2,
         4.884
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Row ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Order ID",
         "type": "\"string\""
        },
        {
         "metadata": "{\"__detected_date_formats\":\"M/d/yyyy\"}",
         "name": "Order Date",
         "type": "\"date\""
        },
        {
         "metadata": "{\"__detected_date_formats\":\"M/d/yyyy\"}",
         "name": "Ship Date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "Ship Mode",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Customer ID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Customer Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Segment",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Country/Region",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "City",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "State/Province",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Postal Code",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Region",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Product ID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Sub-Category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Product Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Sales",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Quantity",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Discount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Profit",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the content of the spark dataframe read from the CSV file\n",
    "display(df1.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a627296-2906-4e36-aa92-c71ca8991b19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Row ID: integer (nullable = true)\n |-- Order ID: string (nullable = true)\n |-- Order Date: date (nullable = true)\n |-- Ship Date: date (nullable = true)\n |-- Ship Mode: string (nullable = true)\n |-- Customer ID: string (nullable = true)\n |-- Customer Name: string (nullable = true)\n |-- Segment: string (nullable = true)\n |-- Country/Region: string (nullable = true)\n |-- City: string (nullable = true)\n |-- State/Province: string (nullable = true)\n |-- Postal Code: string (nullable = true)\n |-- Region: string (nullable = true)\n |-- Product ID: string (nullable = true)\n |-- Category: string (nullable = true)\n |-- Sub-Category: string (nullable = true)\n |-- Product Name: string (nullable = true)\n |-- Sales: double (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- Discount: double (nullable = true)\n |-- Profit: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Check the schema of the spark dataframe with name and data type of each column.\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc67cd6d-fce5-4564-809c-e5ef583f9eed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Extract data from JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "130eef45-be1d-4999-899d-93da008c5364",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The following code will read JSON file using Spark from DBFS into Spark dataframe\n",
    "\n",
    "df2 = spark.read.format(\"json\") \\\n",
    "    .load(\"dbfs:/FileStore/DataFiles/SuperstoreRegions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2069e2fb-2d7c-47e4-9b88-2b7f3880b3f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Region</th><th>RegionalManager</th></tr></thead><tbody><tr><td>West</td><td>Sadie Pawthorne</td></tr><tr><td>East</td><td>Chuck Magee</td></tr><tr><td>Central</td><td>Roxanne Rodriguez</td></tr><tr><td>South</td><td>Fred Suzuki</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "West",
         "Sadie Pawthorne"
        ],
        [
         "East",
         "Chuck Magee"
        ],
        [
         "Central",
         "Roxanne Rodriguez"
        ],
        [
         "South",
         "Fred Suzuki"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Region",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "RegionalManager",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the content of the spark dataframe read from the JSON file\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e7f56b7-80cf-4c80-9e63-74c7ed931a6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Spark dataframe to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b2ed506-3baf-427b-b336-61a9bfcb30a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Check the datatype of the original dataframe, which will be a Spark SQL dataframe\n",
    "print(type(df1)) \n",
    "\n",
    "# Convert that into pandas dataframe\n",
    "pdf1 = df1.toPandas()\n",
    "pdf2 = df2.toPandas()\n",
    "\n",
    "# Check the datatype of the new dataframe, which will be a pandas dataframe\n",
    "print(type(pdf1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83df0d37-f6fa-49d3-91e2-7b4bcc1d2f8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row ID</th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Ship Date</th>\n",
       "      <th>Ship Mode</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Customer Name</th>\n",
       "      <th>Segment</th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>City</th>\n",
       "      <th>...</th>\n",
       "      <th>Postal Code</th>\n",
       "      <th>Region</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sub-Category</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>US-2020-103800</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>2020-01-07</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>DP-13000</td>\n",
       "      <td>Darren Powers</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Houston</td>\n",
       "      <td>...</td>\n",
       "      <td>77095</td>\n",
       "      <td>Central</td>\n",
       "      <td>OFF-PA-10000174</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Paper</td>\n",
       "      <td>\"Message Book Wirebound Four 5 1/2\"\" X 4\"\" For...</td>\n",
       "      <td>16.448</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5.5512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>US-2020-112326</td>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>2020-01-08</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>PO-19195</td>\n",
       "      <td>Phillina Ober</td>\n",
       "      <td>Home Office</td>\n",
       "      <td>United States</td>\n",
       "      <td>Naperville</td>\n",
       "      <td>...</td>\n",
       "      <td>60540</td>\n",
       "      <td>Central</td>\n",
       "      <td>OFF-BI-10004094</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Binders</td>\n",
       "      <td>GBC Standard Plastic Binding Systems Combs</td>\n",
       "      <td>3.540</td>\n",
       "      <td>2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-5.4870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>US-2020-112326</td>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>2020-01-08</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>PO-19195</td>\n",
       "      <td>Phillina Ober</td>\n",
       "      <td>Home Office</td>\n",
       "      <td>United States</td>\n",
       "      <td>Naperville</td>\n",
       "      <td>...</td>\n",
       "      <td>60540</td>\n",
       "      <td>Central</td>\n",
       "      <td>OFF-LA-10003223</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Labels</td>\n",
       "      <td>Avery 508</td>\n",
       "      <td>11.784</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.2717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>US-2020-112326</td>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>2020-01-08</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>PO-19195</td>\n",
       "      <td>Phillina Ober</td>\n",
       "      <td>Home Office</td>\n",
       "      <td>United States</td>\n",
       "      <td>Naperville</td>\n",
       "      <td>...</td>\n",
       "      <td>60540</td>\n",
       "      <td>Central</td>\n",
       "      <td>OFF-ST-10002743</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Storage</td>\n",
       "      <td>SAFCO Boltless Steel Shelving</td>\n",
       "      <td>272.736</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-64.7748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>US-2020-141817</td>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>2020-01-12</td>\n",
       "      <td>Standard Class</td>\n",
       "      <td>MB-18085</td>\n",
       "      <td>Mick Brown</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>United States</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>...</td>\n",
       "      <td>19143</td>\n",
       "      <td>East</td>\n",
       "      <td>OFF-AR-10003478</td>\n",
       "      <td>Office Supplies</td>\n",
       "      <td>Art</td>\n",
       "      <td>Avery Hi-Liter EverBold Pen Style Fluorescent ...</td>\n",
       "      <td>19.536</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.8840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Row ID</th>\n      <th>Order ID</th>\n      <th>Order Date</th>\n      <th>Ship Date</th>\n      <th>Ship Mode</th>\n      <th>Customer ID</th>\n      <th>Customer Name</th>\n      <th>Segment</th>\n      <th>Country/Region</th>\n      <th>City</th>\n      <th>...</th>\n      <th>Postal Code</th>\n      <th>Region</th>\n      <th>Product ID</th>\n      <th>Category</th>\n      <th>Sub-Category</th>\n      <th>Product Name</th>\n      <th>Sales</th>\n      <th>Quantity</th>\n      <th>Discount</th>\n      <th>Profit</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>US-2020-103800</td>\n      <td>2020-01-03</td>\n      <td>2020-01-07</td>\n      <td>Standard Class</td>\n      <td>DP-13000</td>\n      <td>Darren Powers</td>\n      <td>Consumer</td>\n      <td>United States</td>\n      <td>Houston</td>\n      <td>...</td>\n      <td>77095</td>\n      <td>Central</td>\n      <td>OFF-PA-10000174</td>\n      <td>Office Supplies</td>\n      <td>Paper</td>\n      <td>\"Message Book Wirebound Four 5 1/2\"\" X 4\"\" For...</td>\n      <td>16.448</td>\n      <td>2</td>\n      <td>0.2</td>\n      <td>5.5512</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>US-2020-112326</td>\n      <td>2020-01-04</td>\n      <td>2020-01-08</td>\n      <td>Standard Class</td>\n      <td>PO-19195</td>\n      <td>Phillina Ober</td>\n      <td>Home Office</td>\n      <td>United States</td>\n      <td>Naperville</td>\n      <td>...</td>\n      <td>60540</td>\n      <td>Central</td>\n      <td>OFF-BI-10004094</td>\n      <td>Office Supplies</td>\n      <td>Binders</td>\n      <td>GBC Standard Plastic Binding Systems Combs</td>\n      <td>3.540</td>\n      <td>2</td>\n      <td>0.8</td>\n      <td>-5.4870</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>US-2020-112326</td>\n      <td>2020-01-04</td>\n      <td>2020-01-08</td>\n      <td>Standard Class</td>\n      <td>PO-19195</td>\n      <td>Phillina Ober</td>\n      <td>Home Office</td>\n      <td>United States</td>\n      <td>Naperville</td>\n      <td>...</td>\n      <td>60540</td>\n      <td>Central</td>\n      <td>OFF-LA-10003223</td>\n      <td>Office Supplies</td>\n      <td>Labels</td>\n      <td>Avery 508</td>\n      <td>11.784</td>\n      <td>3</td>\n      <td>0.2</td>\n      <td>4.2717</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>US-2020-112326</td>\n      <td>2020-01-04</td>\n      <td>2020-01-08</td>\n      <td>Standard Class</td>\n      <td>PO-19195</td>\n      <td>Phillina Ober</td>\n      <td>Home Office</td>\n      <td>United States</td>\n      <td>Naperville</td>\n      <td>...</td>\n      <td>60540</td>\n      <td>Central</td>\n      <td>OFF-ST-10002743</td>\n      <td>Office Supplies</td>\n      <td>Storage</td>\n      <td>SAFCO Boltless Steel Shelving</td>\n      <td>272.736</td>\n      <td>3</td>\n      <td>0.2</td>\n      <td>-64.7748</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>US-2020-141817</td>\n      <td>2020-01-05</td>\n      <td>2020-01-12</td>\n      <td>Standard Class</td>\n      <td>MB-18085</td>\n      <td>Mick Brown</td>\n      <td>Consumer</td>\n      <td>United States</td>\n      <td>Philadelphia</td>\n      <td>...</td>\n      <td>19143</td>\n      <td>East</td>\n      <td>OFF-AR-10003478</td>\n      <td>Office Supplies</td>\n      <td>Art</td>\n      <td>Avery Hi-Liter EverBold Pen Style Fluorescent ...</td>\n      <td>19.536</td>\n      <td>3</td>\n      <td>0.2</td>\n      <td>4.8840</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 21 columns</p>\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the first 5 records of the pandas dataframe\n",
    "pdf1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91469c6c-096b-4dbb-a04c-d5e82ae3f9c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Region</th>\n",
       "      <th>RegionalManager</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>West</td>\n",
       "      <td>Sadie Pawthorne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>East</td>\n",
       "      <td>Chuck Magee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Central</td>\n",
       "      <td>Roxanne Rodriguez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>South</td>\n",
       "      <td>Fred Suzuki</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Region</th>\n      <th>RegionalManager</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>West</td>\n      <td>Sadie Pawthorne</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>East</td>\n      <td>Chuck Magee</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Central</td>\n      <td>Roxanne Rodriguez</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>South</td>\n      <td>Fred Suzuki</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the first 5 records of the pandas dataframe\n",
    "pdf2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ef55fdb-1400-408d-be36-835ae3d0c970",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Load data from dataframe to databricks table\n",
    "Before we load the data into table, we first have to verify the current database. To do that we convert the cell below to SQL using the %sql magic command and then call the current_database() method, which shows we are currently in a database named as 'default'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6176e744-36a3-4d31-981e-f1ff438099d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>current_database()</th></tr></thead><tbody><tr><td>default</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "default"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"__autoGeneratedAlias\":\"true\"}",
         "name": "current_database()",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select current_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5904db5-a893-4444-8c65-2701a54e465b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2586460941398489>:2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# \u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[43mdf1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moverwrite\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msaveAsTable\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdim_SuperStore_Order\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1520\u001B[0m, in \u001B[0;36mDataFrameWriter.saveAsTable\u001B[0;34m(self, name, format, mode, partitionBy, **options)\u001B[0m\n",
       "\u001B[1;32m   1518\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mformat\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m   1519\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n",
       "\u001B[0;32m-> 1520\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msaveAsTable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Found invalid character(s) among ' ,;{}()\\n\\t=' in the column names of your schema. Please use other characters and try again."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-2586460941398489>:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# \u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mdf1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moverwrite\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msaveAsTable\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdim_SuperStore_Order\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1520\u001B[0m, in \u001B[0;36mDataFrameWriter.saveAsTable\u001B[0;34m(self, name, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m   1518\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mformat\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1519\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n\u001B[0;32m-> 1520\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msaveAsTable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Found invalid character(s) among ' ,;{}()\\n\\t=' in the column names of your schema. Please use other characters and try again.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Found invalid character(s) among ' ,;{}()\\n\\t=' in the column names of your schema. Please use other characters and try again.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write the spark dataframe into databricks table\n",
    "df1.write.mode(\"overwrite\").saveAsTable(\"dim_SuperStore_Order\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "631b6c96-cc59-49e2-81fa-250fabd79499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The above code will return error because the column name in the spark dataframe contains invalid characters like blank space or '/', which are not compatible as column name of databricks table. So we will have to change the column names first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9180af4e-c612-4dd6-b64c-921d063e0733",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[18]: ['Row_ID',\n 'Order_ID',\n 'Order_Date',\n 'Ship_Date',\n 'Ship_Mode',\n 'Customer_ID',\n 'Customer_Name',\n 'Segment',\n 'Country_Or_Region',\n 'City',\n 'State_Or_Province',\n 'Postal_Code',\n 'Region',\n 'Product_ID',\n 'Category',\n 'Sub-Category',\n 'Product_Name',\n 'Sales',\n 'Quantity',\n 'Discount',\n 'Profit']"
     ]
    }
   ],
   "source": [
    "# Convert blank space to _ for all columns\n",
    "renamed_columns = list(map(lambda x: x.replace(\" \", \"_\"), df1.columns))\n",
    "# Convert '/' to '_Or_' for all columns\n",
    "renamed_columns = list(map(lambda x: x.replace(\"/\", \"_Or_\"), renamed_columns))\n",
    "renamed_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30533254-c8ca-40d3-a966-8b2f4dc4b376",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Row_ID: integer (nullable = true)\n |-- Order_ID: string (nullable = true)\n |-- Order_Date: date (nullable = true)\n |-- Ship_Date: date (nullable = true)\n |-- Ship_Mode: string (nullable = true)\n |-- Customer_ID: string (nullable = true)\n |-- Customer_Name: string (nullable = true)\n |-- Segment: string (nullable = true)\n |-- Country_Or_Region: string (nullable = true)\n |-- City: string (nullable = true)\n |-- State_Or_Province: string (nullable = true)\n |-- Postal_Code: string (nullable = true)\n |-- Region: string (nullable = true)\n |-- Product_ID: string (nullable = true)\n |-- Category: string (nullable = true)\n |-- Sub-Category: string (nullable = true)\n |-- Product_Name: string (nullable = true)\n |-- Sales: double (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- Discount: double (nullable = true)\n |-- Profit: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Create a new spark sql dataframe with the renamed columns \n",
    "df1_1 = df1.toDF(*renamed_columns)\n",
    "df1_1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26c122a3-af11-4ae3-9a83-0cc874b5157a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Delete the table dim_SuperStore_Order if it already exists using the below sql command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7835ff7e-7328-46cc-8149-9d88b7ad648a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "drop table if exists default.dim_superstore_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "210af4bd-aa97-4f10-af33-46fd6fd4e2a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the new spark sql dataframe into databricks table\n",
    "df1_1.write.saveAsTable(\"dim_SuperStore_Order\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e65b3d5e-e17e-428f-9943-a35f5de77d92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Validate records in the new table using sql select command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "686368e6-801a-463b-a10a-b2ed63ed719e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Row_ID</th><th>Order_ID</th><th>Order_Date</th><th>Ship_Date</th><th>Ship_Mode</th><th>Customer_ID</th><th>Customer_Name</th><th>Segment</th><th>Country_Or_Region</th><th>City</th><th>State_Or_Province</th><th>Postal_Code</th><th>Region</th><th>Product_ID</th><th>Category</th><th>Sub-Category</th><th>Product_Name</th><th>Sales</th><th>Quantity</th><th>Discount</th><th>Profit</th></tr></thead><tbody><tr><td>1</td><td>US-2020-103800</td><td>2020-01-03</td><td>2020-01-07</td><td>Standard Class</td><td>DP-13000</td><td>Darren Powers</td><td>Consumer</td><td>United States</td><td>Houston</td><td>Texas</td><td>77095</td><td>Central</td><td>OFF-PA-10000174</td><td>Office Supplies</td><td>Paper</td><td>\"Message Book Wirebound Four 5 1/2\"\" X 4\"\" Forms/Pg. 200 Dupl. Sets/Book\"</td><td>16.448</td><td>2</td><td>0.2</td><td>5.5512</td></tr><tr><td>2</td><td>US-2020-112326</td><td>2020-01-04</td><td>2020-01-08</td><td>Standard Class</td><td>PO-19195</td><td>Phillina Ober</td><td>Home Office</td><td>United States</td><td>Naperville</td><td>Illinois</td><td>60540</td><td>Central</td><td>OFF-BI-10004094</td><td>Office Supplies</td><td>Binders</td><td>GBC Standard Plastic Binding Systems Combs</td><td>3.54</td><td>2</td><td>0.8</td><td>-5.487</td></tr><tr><td>3</td><td>US-2020-112326</td><td>2020-01-04</td><td>2020-01-08</td><td>Standard Class</td><td>PO-19195</td><td>Phillina Ober</td><td>Home Office</td><td>United States</td><td>Naperville</td><td>Illinois</td><td>60540</td><td>Central</td><td>OFF-LA-10003223</td><td>Office Supplies</td><td>Labels</td><td>Avery 508</td><td>11.784</td><td>3</td><td>0.2</td><td>4.2717</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "US-2020-103800",
         "2020-01-03",
         "2020-01-07",
         "Standard Class",
         "DP-13000",
         "Darren Powers",
         "Consumer",
         "United States",
         "Houston",
         "Texas",
         "77095",
         "Central",
         "OFF-PA-10000174",
         "Office Supplies",
         "Paper",
         "\"Message Book Wirebound Four 5 1/2\"\" X 4\"\" Forms/Pg. 200 Dupl. Sets/Book\"",
         16.448,
         2,
         0.2,
         5.5512
        ],
        [
         2,
         "US-2020-112326",
         "2020-01-04",
         "2020-01-08",
         "Standard Class",
         "PO-19195",
         "Phillina Ober",
         "Home Office",
         "United States",
         "Naperville",
         "Illinois",
         "60540",
         "Central",
         "OFF-BI-10004094",
         "Office Supplies",
         "Binders",
         "GBC Standard Plastic Binding Systems Combs",
         3.54,
         2,
         0.8,
         -5.487
        ],
        [
         3,
         "US-2020-112326",
         "2020-01-04",
         "2020-01-08",
         "Standard Class",
         "PO-19195",
         "Phillina Ober",
         "Home Office",
         "United States",
         "Naperville",
         "Illinois",
         "60540",
         "Central",
         "OFF-LA-10003223",
         "Office Supplies",
         "Labels",
         "Avery 508",
         11.784,
         3,
         0.2,
         4.2717
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Row_ID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Order_ID",
         "type": "\"string\""
        },
        {
         "metadata": "{\"__detected_date_formats\":\"M/d/yyyy\"}",
         "name": "Order_Date",
         "type": "\"date\""
        },
        {
         "metadata": "{\"__detected_date_formats\":\"M/d/yyyy\"}",
         "name": "Ship_Date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "Ship_Mode",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Customer_ID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Customer_Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Segment",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Country_Or_Region",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "City",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "State_Or_Province",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Postal_Code",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Region",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Product_ID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Sub-Category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Product_Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Sales",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Quantity",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Discount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "Profit",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from default.dim_superstore_order limit 3"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2586460941398496,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02 - Exploratory Data Analysis - Extract Data",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
